{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph-G24VPPkVJ"
      },
      "source": [
        "## **1. Init**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBA5V6fyQHED"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "INPUT_FOLDER = \".\" # TODO: Fill the input path\n",
        "\n",
        "train_file = f\"{INPUT_FOLDER}/train_data.json\" # TODO: Change the train file if needed\n",
        "train_data = []\n",
        "\n",
        "with open(train_file, 'r') as file:\n",
        "    for line in file:\n",
        "        try:\n",
        "            json_obj = json.loads(line)\n",
        "            train_data.append(json_obj)\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Skipping invalid JSON: {line.strip()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcPUqOTuP5X3"
      },
      "source": [
        "## **2. Pre-processing data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4paimkBQPQF"
      },
      "source": [
        "### **Delete unrelated data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ8G0b-fQM5B"
      },
      "outputs": [],
      "source": [
        "modified_train_data = []\n",
        "\n",
        "for json_obj in train_data:\n",
        "    if 'articles' in json_obj:\n",
        "        for article in json_obj['articles']:\n",
        "            if 'article_url' in article:\n",
        "                del article['article_url']\n",
        "            if 'entity_list' in article:\n",
        "                del article['entity_list']\n",
        "            if 'caption_modified' in article:\n",
        "                del article['caption_modified']\n",
        "    if 'maskrcnn_bboxes' in json_obj:\n",
        "        del json_obj['maskrcnn_bboxes']\n",
        "\n",
        "    modified_train_data.append(json_obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEiPX2YbQVx8"
      },
      "source": [
        "### **Normalize data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbrxtm7EQU4X"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "for data_dict in modified_train_data:\n",
        "    if 'articles' in data_dict:\n",
        "        for article in data_dict['articles']:\n",
        "            if 'caption' in article:\n",
        "                article['caption'] = re.sub(r'[^a-zA-Z0-9.\\s]', '', article['caption'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huz_QcghQa4I"
      },
      "source": [
        "### **Remove duplicate data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vr-XZDViyajT"
      },
      "outputs": [],
      "source": [
        "for data_dict in modified_train_data:\n",
        "    if 'articles' in data_dict:\n",
        "        unique_captions = set()\n",
        "        new_articles = []\n",
        "        for article in data_dict['articles']:\n",
        "            if 'caption' in article and article['caption'] not in unique_captions:\n",
        "                unique_captions.add(article['caption'])\n",
        "                new_articles.append(article)\n",
        "        data_dict['articles'] = new_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDmn8jlrQfUv"
      },
      "outputs": [],
      "source": [
        "all_image_path = []\n",
        "\n",
        "for data_dict in modified_train_data:\n",
        "    img_path = data_dict['img_local_path']\n",
        "    all_image_path.append(img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tptE7INERH0o"
      },
      "outputs": [],
      "source": [
        "all_captions = []\n",
        "\n",
        "for data_dict in modified_train_data:\n",
        "    sub_captions = []\n",
        "    if 'articles' in data_dict:\n",
        "        for article in data_dict['articles']:\n",
        "            if 'caption' in article:\n",
        "                sub_captions.append(article['caption'])\n",
        "    all_captions.append(sub_captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc40flH6RPqc"
      },
      "source": [
        "\n",
        "\n",
        "### **Create pairs and labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fArtiMTCRN8v"
      },
      "outputs": [],
      "source": [
        "pairs = []\n",
        "labels = []\n",
        "\n",
        "for i, item in enumerate(modified_train_data):\n",
        "    image_path = item['img_local_path']\n",
        "    captions = [article['caption'] for article in item['articles']]\n",
        "\n",
        "    for caption in all_captions[i]:\n",
        "        pair = [image_path, caption]\n",
        "        pair.append(\"NOOC\")\n",
        "        labels.append(\"NOOC\")\n",
        "        pairs.append(pair)\n",
        "\n",
        "    next_index = (i + 1) % len(modified_train_data)\n",
        "    for caption in all_captions[next_index]:\n",
        "        pair = [image_path, caption]\n",
        "        pair.append(\"OOC\")\n",
        "        labels.append(\"OOC\")\n",
        "        pairs.append(pair)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WVKN3GjJO_A"
      },
      "source": [
        "### **Add index to pairs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWafn7xjJR51"
      },
      "outputs": [],
      "source": [
        "complete_pairs = []\n",
        "\n",
        "for i, item in enumerate(pairs):\n",
        "    image_path = item[0]\n",
        "    original_caption = item[1]\n",
        "    label = item[2]\n",
        "\n",
        "    pair = [i, image_path, original_caption, label]\n",
        "    complete_pairs.append(pair)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-s4ADpXJ8PW"
      },
      "outputs": [],
      "source": [
        "def save_file(pair_list, json_file_path):\n",
        "    with open(json_file_path, 'w') as json_file:\n",
        "        json.dump(pair_list, json_file)\n",
        "\n",
        "    print(f\"Saved to {json_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-IfOwMBJ9dW"
      },
      "outputs": [],
      "source": [
        "save_file(complete_pairs, \"pairs.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjMNeVDeZ8FC"
      },
      "source": [
        "## **3. Generate more captions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZpktEo9KRyz"
      },
      "source": [
        "### **Utils**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RijBLexKQd1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def read_file(json_file_path):\n",
        "    with open(json_file_path, 'r') as json_file:\n",
        "        read_list = json.load(json_file)\n",
        "\n",
        "    return read_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtNHFfX5yaja"
      },
      "outputs": [],
      "source": [
        "def save_file(pair_list, json_file_path):\n",
        "    with open(json_file_path, 'w') as json_file:\n",
        "        json.dump(pair_list, json_file)\n",
        "\n",
        "    print(f\"Saved to {json_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LZqgc89-zvr"
      },
      "outputs": [],
      "source": [
        "def remove_duplicate_sublists(input_list):\n",
        "    seen_items = set()\n",
        "    unique_list = []\n",
        "\n",
        "    for sub_list in input_list:\n",
        "        key = tuple(sub_list[-3:])\n",
        "        if key not in seen_items:\n",
        "            unique_list.append(sub_list)\n",
        "            seen_items.add(key)\n",
        "\n",
        "    return unique_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyov7X6DVGln"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIdA2TpAyajb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP1Bp79OUqum"
      },
      "source": [
        "### **3.1 Generate more NOOC pairs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbmf51VAWHm1"
      },
      "source": [
        "#### **Chatgpt paraphraser on T5 base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TGoDQk2U6no"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2x2vmNxVDK3"
      },
      "outputs": [],
      "source": [
        "def paraphrase(\n",
        "    question,\n",
        "    num_beams=5,\n",
        "    num_beam_groups=5,\n",
        "    num_return_sequences=2,\n",
        "    repetition_penalty=10.0,\n",
        "    diversity_penalty=3.0,\n",
        "    no_repeat_ngram_size=2,\n",
        "    temperature=0.7,\n",
        "    max_length=128\n",
        "):\n",
        "    input_ids = tokenizer(\n",
        "        f'paraphrase: {question}',\n",
        "        return_tensors=\"pt\", padding=\"longest\",\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "    ).input_ids.to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n",
        "        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        num_beams=num_beams, num_beam_groups=num_beam_groups,\n",
        "        max_length=max_length, diversity_penalty=diversity_penalty\n",
        "    )\n",
        "\n",
        "    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hShEF-ASWQP6"
      },
      "source": [
        "#### **Implement in pairs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ueSwU8ZKZXR"
      },
      "outputs": [],
      "source": [
        "pairs = read_file(\"pairs.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukRQ-otaWgwu"
      },
      "outputs": [],
      "source": [
        "more_nooc_pairs = []\n",
        "\n",
        "for i, item in enumerate(pairs):\n",
        "    index = item[0]\n",
        "    image_path = item[1]\n",
        "    original_caption = item[2]\n",
        "    label = item[3]\n",
        "\n",
        "    if (label == 'NOOC'):\n",
        "        more_nooc_pairs.append(item)\n",
        "        gen_list = paraphrase(original_caption)\n",
        "        for sentences in gen_list:\n",
        "            pair = [index, image_path, sentences, label]\n",
        "            more_nooc_pairs.append(pair)\n",
        "\n",
        "    elif (label == 'OOC'):\n",
        "        pair = [index, image_path, original_caption, label]\n",
        "        more_nooc_pairs.append(pair)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE0DeA5TZFRN"
      },
      "outputs": [],
      "source": [
        "non_duplicated_more_nooc_pairs = remove_duplicate_sublists(more_nooc_pairs)\n",
        "save_file(non_duplicated_more_nooc_pairs, \"more_nooc_pairs.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJOSt52nuCJ_"
      },
      "source": [
        "### **3.2 Generate more OOC pairs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1zzBFlV6d84"
      },
      "source": [
        "#### **GPT to generate paragraph from captions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnnKuzJv-Qtc"
      },
      "outputs": [],
      "source": [
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fdq1E8hw-TQv"
      },
      "outputs": [],
      "source": [
        "def generate_paragraph(prompt):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    gen_tokens = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        max_length=100,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
        "\n",
        "    return gen_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYZ1ZV0XAWt0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def preprocess_paragraph(paragraph):\n",
        "    paragraph = re.sub(r'[^a-zA-Z0-9.\\s]', '', paragraph)\n",
        "    sentences = re.split(r'\\.\\ (?=[A-Z])|.\\n\\n', paragraph)\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence and len(sentence)>=35]\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y01aFkPSR4Bi"
      },
      "source": [
        "#### **Implement in pairs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyyKv_cASc8b"
      },
      "outputs": [],
      "source": [
        "pairs = read_file(\"more_nooc_pairs.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Opa6dWnwLCos"
      },
      "outputs": [],
      "source": [
        "more_ooc_pairs = []\n",
        "\n",
        "for i, item in enumerate(pairs):\n",
        "    index = item[0]\n",
        "    image_path = item[1]\n",
        "    original_caption = item[2]\n",
        "    label = item[3]\n",
        "\n",
        "    if (label == 'OOC'):\n",
        "        more_ooc_pairs.append(item)\n",
        "\n",
        "        gen_paragraph = generate_paragraph(original_caption)\n",
        "        processed_sentences = preprocess_paragraph(gen_paragraph)\n",
        "\n",
        "        for sentence in processed_sentences:\n",
        "            pair = [index, image_path, sentence, label]\n",
        "            more_ooc_pairs.append(pair)\n",
        "\n",
        "    elif (label == 'NOOC'):\n",
        "        pair = [index, image_path, original_caption, label]\n",
        "        more_ooc_pairs.append(pair)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B_6CezkBKtC"
      },
      "outputs": [],
      "source": [
        "non_duplicated_more_ooc_pairs = remove_duplicate_sublists(more_ooc_pairs)\n",
        "save_file(non_duplicated_more_ooc_pairs, \"more_ooc_nooc_pairs.json\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qcPUqOTuP5X3",
        "e4paimkBQPQF",
        "CEiPX2YbQVx8",
        "huz_QcghQa4I",
        "Wc40flH6RPqc",
        "4WVKN3GjJO_A",
        "OZpktEo9KRyz",
        "Kbmf51VAWHm1",
        "hShEF-ASWQP6",
        "h1zzBFlV6d84",
        "Y01aFkPSR4Bi"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}